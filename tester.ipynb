{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from config import *\n",
    "# service_config = ServiceConfig()\n",
    "# model_config = ModelConfig()\n",
    "from arguments import args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import data_source_factory\n",
    "data = data_source_factory(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'game_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# make interaction data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessor_factory\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpreprocessor_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\github\\recommendation_pipeline\\dataset\\__init__.py:16\u001b[0m, in \u001b[0;36mpreprocessor_factory\u001b[1;34m(args, data)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessor_factory\u001b[39m(data, args):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame_name\u001b[49m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon_games\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     17\u001b[0m         preprocesser \u001b[38;5;241m=\u001b[39m AmazonGamesPreprocessor(data)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mgame_name\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteam_games\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gnsdl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6293\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6287\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6288\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6289\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6290\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6291\u001b[0m ):\n\u001b[0;32m   6292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'game_name'"
     ]
    }
   ],
   "source": [
    "# make interaction data\n",
    "from dataset import preprocessor_factory\n",
    "preprocessor_factory(data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle('data/steam_games_interaction_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from dataset.base import BaseDataset\n",
    "class EASEDataset(BaseDataset):\n",
    "    \"\"\"\n",
    "    MatrixDataSet 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, data, args):\n",
    "        self.args = args\n",
    "        self.data = self.load(f'data/{args.game_name}_interaction_data.pkl')\n",
    "        self.data['user_idx'] = self.data['user_id'].map(self.encode(self.data['user_id']))\n",
    "        self.data['item_idx'] = self.data['item_id'].map(self.encode(self.data['item_id']))\n",
    "\n",
    "        self.num_users = self.data['user_id'].nunique()\n",
    "        self.num_items = self.data['item_id'].nunique()\n",
    "        args.num_users = self.num_users\n",
    "        args.num_items = self.num_items\n",
    "        self.users = self.data['user_idx'].unique()\n",
    "        \n",
    "        self.train_data = {}\n",
    "        self.valid_data = {}\n",
    "        self.user_item_dict = self._make_user_item_dict()\n",
    "\n",
    "    def encode(self, feature) -> dict:\n",
    "        return {v:i for i, v in enumerate(feature.unique())}\n",
    "    \n",
    "    def decode(self, feature) -> dict:\n",
    "        return {i:v for i, v in enumerate(feature.unique())}\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return super().get_feature_names()\n",
    "    \n",
    "    def load(self, path):\n",
    "        data = pd.read_pickle(path)\n",
    "        return data\n",
    "    \n",
    "    def _make_user_item_dict(self) -> dict:\n",
    "        user_item_dict = defaultdict(list)\n",
    "        for user, item in zip(self.data['user_idx'], self.data['item_idx']):\n",
    "            user_item_dict[user].append(item)\n",
    "        return user_item_dict\n",
    "    \n",
    "    def train_valid_split(self, valid_sample=5):\n",
    "        assert self.user_item_dict is not None, 'user_item_dict is None. Run make_user_item_dict() first.'\n",
    "        for user in self.user_item_dict:\n",
    "            total = self.user_item_dict[user]\n",
    "            valid = np.random.choice(self.user_item_dict[user], valid_sample, replace=True)\n",
    "            train = np.setdiff1d(total, valid)\n",
    "            self.train_data[user] = list(train)\n",
    "            self.valid_data[user] = list(valid)\n",
    "        return None\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def make_matrix(self, user_list, train = True):\n",
    "        \"\"\"\n",
    "        user_item_dict를 바탕으로 행렬 생성\n",
    "        \"\"\"\n",
    "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
    "        for idx, user in enumerate(user_list):\n",
    "            if train:\n",
    "                mat[idx, self.train_data[user.item()]] = 1\n",
    "            else:\n",
    "                mat[idx, self.train_data[user.item()] + self.valid_data[user.item()]] = 1\n",
    "                # 왜 train의 index를 더해줄까?\n",
    "        return mat\n",
    "\n",
    "    def make_sparse_matrix(self):\n",
    "        X = sp.dok_matrix((self.num_users, self.num_items), dtype=np.float32)\n",
    "        for user in self.train_data.keys():\n",
    "            item_list = self.train_data[user]\n",
    "            X[user, item_list] = 1.0\n",
    "                \n",
    "        return X.tocsr()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EASEDataset(data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_valid_split(valid_sample=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12393x5155 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.make_sparse_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.base import BaseDataset\n",
    "\n",
    "class EASEDataset(BaseDataset):\n",
    "    def __init__(self, data, model_config):\n",
    "        self.data = data\n",
    "        self.model_config = model_config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, item_id, label = self.data[idx]\n",
    "        return user_id, item_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model input \n",
    "# input : interaction data\n",
    "# output : train, test data  e.g user : [item1, item2, item3, ...]\n",
    "from dataset import dataset_factory\n",
    "dataset = dataset_factory(service_config.game_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import dataloader_factory\n",
    "dataloader = dataloader_factory(dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from abc import ABC, abstractmethod\n",
    "class BaseDataLoader(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, dataset, args):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __iter__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "class PytorchDataLoader(BaseDataLoader):\n",
    "\n",
    "    def __init__(self, dataset, args):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = args.batch_size\n",
    "        # self.shuffle = args.shuffle\n",
    "        # self.num_workers = args.num_workers\n",
    "        self.data_loader = data_utils.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "class MatrixDataLoader(BaseDataLoader):\n",
    "\n",
    "    def __init__(self, dataset, args):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = MatrixDataLoader(dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASE():\n",
    "    def __init__(self, X):\n",
    "        self.X = self._convert_sp_mat_to_sp_tensor(X)\n",
    "    \n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32) # tocoo : COOrdinate format으로 변환 / COOrdinate format : 희소행렬을 나타내기 위한 방법\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col])) # row index, col index\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to('cuda')\n",
    "        return res\n",
    "\n",
    "    def fit(self, reg):\n",
    "        '''\n",
    "        진짜 정말 간단한 식으로 모델을 만듬\n",
    "        '''\n",
    "        G = self.X.to_dense().t() @ self.X.to_dense() # X^T * X\n",
    "        diagIndices = torch.eye(G.shape[0]) == 1 # 대각선 index\n",
    "        G[diagIndices] += reg  # regularization\n",
    "\n",
    "        P = G.inverse() # inverse matrix\n",
    "        B = P / (-1 * P.diag()) # B = -P / P_ii\n",
    "        B[diagIndices] = 0 # 대각선은 0으로 만들어줌 / 왜냐하면 자기 자신과의 similarity는 0이기 때문\n",
    "\n",
    "        self.pred = self.X.to_dense() @ B  # X * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EASE(dataset.make_sparse_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import model_factory\n",
    "model = model_factory(model_name='AutoRec', input_dim=dataset.num_items, hidden_dim=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from runner.metric import get_ndcg, get_hit\n",
    "\n",
    "class BaseRunner(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for training and testing PyTorch models.\n",
    "    하위 클래스에서 필수적으로 구현해야하는 메소드는 아래와 같습니다.\n",
    "    - train : epochs만큼 모델을 학습할 수 있는 기능\n",
    "    - train_one_epoch : 1 epoch만큼 모델을 학습할 수 있는 기능\n",
    "    - evaluate : 모델 성능을 모니터링하는 기능\n",
    "    - inference : 학습된 모델을 사용하여 추론을 수행하는 기능\n",
    "    - save : 모델의 checkpoint를 저장하는 기능\n",
    "    - load : 모델의 checkpoint를 불러오는 기능\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataloader, args):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.args = args \n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, train_loader, epoch):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_one_epoch(self, train_loader, epoch):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, val_loader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, user_ids):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, path):\n",
    "        pass\n",
    "\n",
    "\n",
    "class EASERunner(BaseRunner):\n",
    "\n",
    "    def __init__(self, model, dataloader, args):\n",
    "        super().__init__(model, dataloader, args)\n",
    "        self.model = None\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "        # self.lr = args.lr\n",
    "        self.device = args.device\n",
    "        self.topk = args.topk\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.reg = args.reg\n",
    "\n",
    "        # self.optimizer = self._create_optimizer(args.optimizer)\n",
    "        # self.criterion = self._create_criterion(args.criterion)\n",
    "\n",
    "    def train(self):\n",
    "        X = self.dataset.make_sparse_matrix()\n",
    "        for reg in self.reg:\n",
    "            self.train_one_epoch(X, reg)\n",
    "            NDCG, HIT = self.evaluate()\n",
    "            print(f'NDCG:{NDCG} / HIT: {HIT}')\n",
    "        return None\n",
    "        \n",
    "    def train_one_epoch(self, X, reg):\n",
    "        self.model = EASE()\n",
    "        self.model.X = self.model._convert_sp_mat_to_sp_tensor(X)\n",
    "        self.model.fit(reg)\n",
    "        return None\n",
    "    \n",
    "    def evaluate(self):\n",
    "        NDCG = 0.0 \n",
    "        HIT = 0.0 \n",
    "        pred = self.model.pred.cpu()\n",
    "        X = self.dataset.make_sparse_matrix().toarray()\n",
    "        mat = torch.from_numpy(X)\n",
    "\n",
    "        pred[mat == 1] = -1\n",
    "        pred = pred.argsort(dim = 1)\n",
    "\n",
    "        for user, rec1 in tqdm(enumerate(pred)):\n",
    "            uv = dataset.valid_data[user]\n",
    "\n",
    "            # ranking\n",
    "            up = rec1[-5:].cpu().numpy().tolist()[::-1]\n",
    "\n",
    "            NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
    "            HIT += get_hit(pred_list = up, true_list = uv)\n",
    "\n",
    "        NDCG /= len(dataset.train_data)\n",
    "        HIT /= len(dataset.train_data)\n",
    "\n",
    "        return NDCG, HIT\n",
    "    \n",
    "    def inference(self, user_ids):\n",
    "\n",
    "        user_list = [user_ids]\n",
    "        mat = self.dataset.get_matrix(user_list, trainYn=False).to(self.device)\n",
    "        recon_mat = self.model(mat)\n",
    "        recon_mat = recon_mat.softmax(dim = 1)\n",
    "        recon_mat[mat == 1] = -1.\n",
    "        rec_list = recon_mat.argsort(dim = 1)\n",
    "        rec_list = rec_list[0].cpu().numpy().tolist()\n",
    "        rec_list = rec_list[-self.args.topk:]\n",
    "        return rec_list\n",
    "\n",
    "    def _create_optimizer(self, optimizer):\n",
    "        if optimizer == 'adam':\n",
    "            return optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer == 'sgd':\n",
    "            return optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer')\n",
    "\n",
    "    def _create_criterion(self, criterion):\n",
    "        if criterion == 'mse':\n",
    "            return nn.MSELoss()\n",
    "        elif criterion == 'ce':\n",
    "            return nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise ValueError('Invalid criterion')\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f'Model saved to {path}')\n",
    "        return None\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        print(f'Model loaded from {path}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_epochs =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EASERunner(model, dataloader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 102420.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.24149737904033453 / HIT: 0.07087872185911483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 97582.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.27899916360934285 / HIT: 0.08143306705398337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 102421.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.3370102134099699 / HIT: 0.09700637456628933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 87273.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.3776437045632483 / HIT: 0.10886790930364154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 87891.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.3836400787497139 / HIT: 0.11056241426612042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 99145.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.3762844409608466 / HIT: 0.10831921245864842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12393it [00:00, 94603.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:0.3699566464715221 / HIT: 0.10662470749616958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runner import runner_factory\n",
    "runner = runner_factory(model, dataloader, optimizer, criterion, lr, device, dataset, scheduler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
